{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH_TO_REPO\"] = \"/Users/stevie/repos/lingo_kit_data\"\n",
    "# os.environ[\"PATH_TO_REPO\"] = \"/home/ubuntu/busy_bees/lingo_kit_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in environment variable\n",
    "import os\n",
    "PATH_TO_REPO = os.getenv('PATH_TO_REPO')\n",
    "PATH_TO_REPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(PATH_TO_REPO)\n",
    "from utils.chatgpt.generate_with_chatgpt import generate_csv\n",
    "from utils.csv_helper import get_all_terms_df, get_all_csv_files_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = os.path.join(PATH_TO_REPO, 'dataframes/old_dataframes/spotify_lessons.csv')\n",
    "# path = os.path.join(PATH_TO_REPO, 'dataframes/old_dataframes/foundational_words.csv')\n",
    "path = os.path.join(PATH_TO_REPO, 'dataframes/old_dataframes/category_data.csv')\n",
    "assert(os.path.exists(path))\n",
    "df = pd.read_csv(path)\n",
    "len(df), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = '''!()-[]{};:\"\\,<>./?@#$%^&*_~'''\n",
    "word_set = set()\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    words = row['italian_term'].lower().split(' ')\n",
    "    for word in words:\n",
    "        for p in punctuations:\n",
    "            word = word.replace(p, '')\n",
    "        if word != '':\n",
    "            word_set.add(word)\n",
    "len(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_words = list(sorted(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "all_df = get_all_terms_df()\n",
    "for term_it in tqdm(italian_words):\n",
    "    if term_it.lower() in all_df['term_italian'].str.lower().values:\n",
    "        print(f\"Skipping term '{term_it}' as it already exists in the dataset.\")\n",
    "    else:\n",
    "        new_list.append(term_it)\n",
    "len(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_words = sorted(new_list)\n",
    "italian_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(italian_words))\n",
    "italian_words = [x for x in italian_words if 'mille' not in x]\n",
    "print(len(italian_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = ['guardia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_list = []\n",
    "generated_files_list = []\n",
    "cost_info_list = []\n",
    "for term_it in tqdm(italian_words):\n",
    "    if term_it in blacklist:\n",
    "        print(f\"Skipping blacklisted term '{term_it}'.\")\n",
    "        continue\n",
    "\n",
    "    all_df = get_all_terms_df()\n",
    "    if term_it.lower() in all_df['term_italian'].str.lower().values:\n",
    "        print(f\"Skipping term '{term_it}' as it already exists in the dataset.\")\n",
    "        continue\n",
    "\n",
    "    attempt_count = 0\n",
    "    for attempt_count in range(3):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt_count + 1} for term: {term_it}\")\n",
    "            response, generated_files, cost_info = generate_csv(\n",
    "                italian_term=term_it,\n",
    "                model='gpt-5-mini',\n",
    "                # model='gpt-5',\n",
    "                reasoning_effort='medium',\n",
    "            )\n",
    "            response_list.append(response)\n",
    "            generated_files_list.extend(generated_files)\n",
    "            cost_info_list.append(cost_info)\n",
    "            print(f\"cost of this call: {cost_info['total_cost']}\")\n",
    "            break  # Exit the retry loop if successful\n",
    "        except pd.errors.ParserError as e:\n",
    "            print(f\"ParserError on attempt {attempt_count + 1} for term '{term_it}': {e}\")\n",
    "            if attempt_count == 2:\n",
    "                print(f\"Failed to process term '{term_it}' after 3 attempts.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {attempt_count + 1} for term '{term_it}': {e}\")\n",
    "            if attempt_count == 2:\n",
    "                print(f\"Failed to process term '{term_it}' after 3 attempts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
