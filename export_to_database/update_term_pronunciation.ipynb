{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH_TO_REPO'] = '/Users/stevie/repos/lingo_kit_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = os.environ['PATH_TO_REPO']\n",
    "assert(os.path.exists(repo_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Term Pronunciation\n",
    "\n",
    "Loads all CSVs under `dataframes/dataframes_by_pos`, collects unique `term_italian` values and their `pronunciation`, then PATCHes each Term via the API endpoint `terms/by-term-italian/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('http://127.0.0.1:8000', '/api/terms/by-term-italian/')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directory containing per-POS CSVs\n",
    "DATA_DIR = Path(os.path.join(repo_path, 'dataframes/dataframes_by_pos'))\n",
    "\n",
    "# Base URL of your API (no trailing slash)\n",
    "API_BASE_URL = os.getenv('LINGOKIT_API_BASE_URL', 'http://127.0.0.1:8000')\n",
    "# Endpoint path for patching by exact term_italian\n",
    "TERMS_BY_TERM_ITALIAN_PATH = os.getenv('LINGOKIT_TERMS_BY_TERM_PATH', '/api/terms/by-term-italian/')\n",
    "\n",
    "# Auth token (Token or JWT). If it contains dots, 'Bearer' will be used; otherwise 'Token'.\n",
    "AUTH_TOKEN = ''  # explicitly unused per request\n",
    "AUTH_SCHEME_OVERRIDE = ''  # explicitly unused per request\n",
    "# Explicit Basic Auth credentials (per request)\n",
    "USERNAME = 'stevie'\n",
    "PASSWORD = 'lingokit2025!'\n",
    "\n",
    "# Safety toggle. Set to False to actually PATCH the API.\n",
    "DRY_RUN = False\n",
    "\n",
    "def build_session():\n",
    "    s = requests.Session()\n",
    "    s.headers['Accept'] = 'application/json'\n",
    "    s.headers['Content-Type'] = 'application/json'\n",
    "    # Always use HTTP Basic with explicit credentials\n",
    "    s.auth = (USERNAME, PASSWORD)\n",
    "    return s\n",
    "\n",
    "def terms_patch_url():\n",
    "    return f\"{API_BASE_URL.rstrip('/')}\" + TERMS_BY_TERM_ITALIAN_PATH\n",
    "\n",
    "API_BASE_URL, TERMS_BY_TERM_ITALIAN_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(801,\n",
       " [PosixPath('/Users/stevie/repos/lingo_kit_data/dataframes/dataframes_by_pos/adjective/addolorato.csv'),\n",
       "  PosixPath('/Users/stevie/repos/lingo_kit_data/dataframes/dataframes_by_pos/adjective/adulto.csv'),\n",
       "  PosixPath('/Users/stevie/repos/lingo_kit_data/dataframes/dataframes_by_pos/adjective/aereo.csv'),\n",
       "  PosixPath('/Users/stevie/repos/lingo_kit_data/dataframes/dataframes_by_pos/adjective/aggiunto.csv'),\n",
       "  PosixPath('/Users/stevie/repos/lingo_kit_data/dataframes/dataframes_by_pos/adjective/alto.csv')])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files = sorted(DATA_DIR.rglob('*.csv'))\n",
    "len(csv_files), csv_files[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build term -> pronunciation mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning CSVs: 100%|██████████| 801/801 [00:00<00:00, 1439.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8608, 100, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_to_pron = {}  # term_italian -> pronunciation\n",
    "conflicts = {}      # term_italian -> set of differing pronunciations\n",
    "missing_pron = set()\n",
    "\n",
    "for path in tqdm(csv_files, desc='Scanning CSVs'):\n",
    "    try:\n",
    "        df = pd.read_csv(path, dtype=str, keep_default_na=False, na_filter=False, encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        print(f'[WARN] Failed to read {path}: {e}')\n",
    "        continue\n",
    "\n",
    "    if 'term_italian' not in df.columns:\n",
    "        print(f'[WARN] term_italian column missing in {path}')\n",
    "        continue\n",
    "    if 'pronunciation' not in df.columns:\n",
    "        print(f'[WARN] pronunciation column missing in {path}')\n",
    "        continue\n",
    "\n",
    "    for term, pron in zip(df['term_italian'], df['pronunciation']):\n",
    "        t = (term or '').strip()\n",
    "        p = (pron or '').strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        if not p:\n",
    "            missing_pron.add(t)\n",
    "            # don't overwrite an existing non-empty mapping\n",
    "            continue\n",
    "        if t not in term_to_pron:\n",
    "            term_to_pron[t] = p\n",
    "        else:\n",
    "            if term_to_pron[t] != p:\n",
    "                # record conflict; keep the first seen\n",
    "                conflicts.setdefault(t, set()).update({term_to_pron[t], p})\n",
    "\n",
    "len(term_to_pron), len(conflicts), len(missing_pron)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are conflicts above, the first seen pronunciation will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auth method: Basic (user=stevie)\n",
      "PATCH URL: http://127.0.0.1:8000/api/terms/by-term-italian/\n"
     ]
    }
   ],
   "source": [
    "session = build_session()\n",
    "print('Auth method: Basic (user=stevie)')\n",
    "url = terms_patch_url()\n",
    "print('PATCH URL:', url)\n",
    "\n",
    "def patch_term_pronunciation(term_italian: str, pronunciation: str):\n",
    "    params = {'term_italian': term_italian}\n",
    "    payload = {'pronunciation': pronunciation}\n",
    "    if DRY_RUN:\n",
    "        return {'dry_run': True, 'term_italian': term_italian, 'pronunciation': pronunciation}\n",
    "    resp = session.patch(url, params=params, json=payload, timeout=30)\n",
    "    return resp\n",
    "\n",
    "# quick smoke test (disabled by default)\n",
    "# _ = patch_term_pronunciation('il treno', 'il tré-no')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patching terms: 100%|██████████| 8608/8608 [4:02:22<00:00,  1.69s/it]     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'updated': 4098, 'skipped_missing_pron': 0, 'errors': 4510}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {\n",
    "    'updated': 0,\n",
    "    'skipped_missing_pron': 0,\n",
    "    'errors': 0,\n",
    "}\n",
    "errors = []\n",
    "\n",
    "for term, pron in tqdm(term_to_pron.items(), desc='Patching terms'):\n",
    "    if not pron:\n",
    "        results['skipped_missing_pron'] += 1\n",
    "        continue\n",
    "    try:\n",
    "        resp = patch_term_pronunciation(term, pron)\n",
    "        if DRY_RUN:\n",
    "            results['updated'] += 1\n",
    "            continue\n",
    "        if getattr(resp, 'status_code', None) in (200, 202):\n",
    "            results['updated'] += 1\n",
    "        else:\n",
    "            results['errors'] += 1\n",
    "            body = getattr(resp, 'text', '')\n",
    "            errors.append({'term': term, 'status': resp.status_code, 'body': body[:500]})\n",
    "    except Exception as e:\n",
    "        results['errors'] += 1\n",
    "        errors.append({'term': term, 'error': str(e)})\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conflicts: 100\n",
      "- certo -> ['CHEHR-toh', 'CHER-toh']\n",
      "- lontano -> ['lon-TAH-noh', 'lohn-TAH-noh']\n",
      "- pronto -> ['PRON-toh', 'PROHN-toh']\n",
      "- spesso -> ['SPEHS-soh', 'SPEH-soh']\n",
      "- però -> ['pehr-OH', 'peh-ROH']\n",
      "Missing pronunciations (not patched): 0\n",
      "Errors: 4510\n",
      "{'term': 'aereo', 'status': 404, 'body': '{\"detail\":\"No Term matches the given query.\"}'}\n",
      "{'term': 'aerea', 'status': 404, 'body': '{\"detail\":\"No Term matches the given query.\"}'}\n",
      "{'term': 'aerei', 'status': 404, 'body': '{\"detail\":\"No Term matches the given query.\"}'}\n",
      "{'term': 'aeree', 'status': 404, 'body': '{\"detail\":\"No Term matches the given query.\"}'}\n",
      "{'term': \"dall'altro\", 'status': 404, 'body': '{\"detail\":\"No Term matches the given query.\"}'}\n",
      "{'term': \"dall'altra\", 'status': 404, 'body': '{\"detail\":\"No Term matches the given query.\"}'}\n",
      "{'term': \"d'altro\", 'status': 404, 'body': '{\"detail\":\"No Term matches the given query.\"}'}\n",
      "{'term': \"l'altra\", 'status': 404, 'body': '{\"detail\":\"No Term matches the given query.\"}'}\n",
      "{'term': \"l'altro\", 'status': 404, 'body': '{\"detail\":\"No Term matches the given query.\"}'}\n",
      "{'term': 'gli altri', 'status': 404, 'body': '{\"detail\":\"No Term matches the given query.\"}'}\n"
     ]
    }
   ],
   "source": [
    "print('Conflicts:', len(conflicts))\n",
    "if conflicts:\n",
    "    # show a few examples\n",
    "    for i, (t, ps) in enumerate(conflicts.items()):\n",
    "        if i >= 5: break\n",
    "        print('-', t, '->', list(ps))\n",
    "print('Missing pronunciations (not patched):', len(missing_pron))\n",
    "print('Errors:', len(errors))\n",
    "if errors:\n",
    "    for i, e in enumerate(errors[:10]):\n",
    "        print(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
