{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export DataFrames to Backend\n",
    "\n",
    "This notebook scans `dataframes/dataframes_by_pos` for CSVs, loads rows, and publishes them to the backend via the API.\n",
    "\n",
    "Notes:\n",
    "- Ignores the `is_base` column.\n",
    "- Uses `tqdm` to show progress for files and rows.\n",
    "- Creates topics, base lemmas, terms, baselemma-topic links, and term translations as needed.\n",
    "- See `export_to_database/example_database_api_calls.ipynb` for reference API patterns.\n",
    "\n",
    "Configuration via env vars (optional):\n",
    "- `LINGOKIT_BASE_URL` (default: `http://127.0.0.1:8000`)\n",
    "- `LINGOKIT_AUTH_USER` (default: `stevie`)\n",
    "- `LINGOKIT_AUTH_PASS` (default: `lingokit2025!`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH_TO_REPO\"] = \"/Users/stevie/repos/lingo_kit_data\"\n",
    "# os.environ[\"PATH_TO_REPO\"] = \"/home/ubuntu/busy_bees/lingo_kit_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/stevie/repos/lingo_kit_data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in environment variable\n",
    "import os\n",
    "PATH_TO_REPO = os.getenv('PATH_TO_REPO')\n",
    "PATH_TO_REPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevie/repos/lingo_kit_data/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'list_reverseiterator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 241\u001b[39m\n\u001b[32m    239\u001b[39m csv_files = \u001b[38;5;28msorted\u001b[39m(iter_csv_files(DATA_DIR))\n\u001b[32m    240\u001b[39m csv_files = \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mlist\u001b[39m(csv_files))  \u001b[38;5;66;03m# Process in reverse order to prioritize certain files\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcsv_files\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m CSV files under \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    242\u001b[39m created_tt_ids = []\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m csv_path \u001b[38;5;129;01min\u001b[39;00m tqdm(csv_files, desc=\u001b[33m'\u001b[39m\u001b[33mCSV files\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'list_reverseiterator' has no len()"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Config ---\n",
    "BASE_URL = os.getenv('LINGOKIT_BASE_URL', 'http://127.0.0.1:8000')\n",
    "AUTH = (\n",
    "    os.getenv('LINGOKIT_AUTH_USER', 'stevie'),\n",
    "    os.getenv('LINGOKIT_AUTH_PASS', 'lingokit2025!'),\n",
    ")\n",
    "DATA_DIR = os.path.join(PATH_TO_REPO, 'dataframes', 'dataframes_by_pos')\n",
    "assert os.path.isdir(DATA_DIR), f'Expected directory not found: {DATA_DIR}'\n",
    "\n",
    "# --- Value normalization ---\n",
    "ALLOWED_POS = {'adj', 'adv', 'art', 'conj', 'det', 'noun', 'prep', 'pron', 'verb'}\n",
    "POS_MAP: Dict[str, str] = {\n",
    "    'adj': 'adj', 'adjective': 'adj', 'adjectives': 'adj',\n",
    "    'adv': 'adv', 'adverb': 'adv', 'adverbs': 'adv',\n",
    "    'art': 'art', 'article': 'art', 'articles': 'art',\n",
    "    'conj': 'conj', 'conjunction': 'conj', 'conjunctions': 'conj',\n",
    "    'det': 'det', 'determiner': 'det', 'determiners': 'det',\n",
    "    'noun': 'noun', 'nouns': 'noun',\n",
    "    'prep': 'prep', 'preposition': 'prep', 'prepositions': 'prep',\n",
    "    'pron': 'pron', 'pronoun': 'pron', 'pronouns': 'pron',\n",
    "    'verb': 'verb', 'verbs': 'verb',\n",
    "}\n",
    "GENDER_MAP: Dict[str, str] = {'masculine': 'm', 'feminine': 'f', 'n/a': 'n/a', 'none': 'n/a', '': 'n/a'}\n",
    "PLURALITY_MAP: Dict[str, str] = {'singular': 's', 'plural': 'p', 'n/a': 'n/a', 'none': 'n/a', '': 'n/a'}\n",
    "\n",
    "def normalize_pos(value: Optional[str]) -> Optional[str]:\n",
    "    if value is None:\n",
    "        return None\n",
    "    key = str(value).strip().lower()\n",
    "    if not key:\n",
    "        return None\n",
    "    return POS_MAP.get(key, key if key in ALLOWED_POS else None)\n",
    "\n",
    "def normalize_gender(value: Optional[str]) -> str:\n",
    "    key = 'n/a' if value is None else str(value).strip().lower()\n",
    "    return GENDER_MAP.get(key, key if key in {'m', 'f', 'n/a'} else 'n/a')\n",
    "\n",
    "def normalize_plurality(value: Optional[str]) -> str:\n",
    "    key = 'n/a' if value is None else str(value).strip().lower()\n",
    "    return PLURALITY_MAP.get(key, key if key in {'s', 'p', 'n/a'} else 'n/a')\n",
    "\n",
    "# --- Caches ---\n",
    "topic_cache: Dict[str, int] = {}\n",
    "baselemma_cache: Dict[Tuple[str, str], int] = {}\n",
    "term_cache: Dict[str, int] = {}\n",
    "baselemma_topic_cache = set()\n",
    "position_tracker = defaultdict(int)  # tracks per (base_it, base_en) position\n",
    "\n",
    "def _extract_results(payload: Any):\n",
    "    if isinstance(payload, dict):\n",
    "        results = payload.get('results')\n",
    "        if isinstance(results, list):\n",
    "            return results\n",
    "        return []\n",
    "    if isinstance(payload, list):\n",
    "        return payload\n",
    "    return []\n",
    "\n",
    "# --- HTTP error handling helpers ---\n",
    "def _redact_auth(auth):\n",
    "    try:\n",
    "        user, _ = auth or (None, None)\n",
    "        return f\"{user}:***\" if user else None\n",
    "    except Exception:\n",
    "        return \"***\"\n",
    "\n",
    "def _json_preview(text, limit=2000):\n",
    "    try:\n",
    "        import json as _json\n",
    "        obj = _json.loads(text)\n",
    "        return _json.dumps(obj, ensure_ascii=False, indent=2)[:limit]\n",
    "    except Exception:\n",
    "        return (text or \"\")[:limit]\n",
    "\n",
    "def raise_for_status_detailed(\n",
    "    resp: requests.Response,\n",
    "    *,\n",
    "    method: Optional[str] = None,\n",
    "    url: Optional[str] = None,\n",
    "    params: Optional[Dict[str, Any]] = None,\n",
    "    json_body: Optional[Dict[str, Any]] = None,\n",
    "    data: Optional[Any] = None,\n",
    "    auth: Optional[Any] = None,\n",
    "    context: Optional[str] = None,\n",
    ") -> None:\n",
    "    \"\"\"Raise an HTTPError with rich details if response is not OK.\n",
    "\n",
    "    Includes request/response metadata and a preview of the response body.\n",
    "    \"\"\"\n",
    "    if resp.status_code < 400:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        req = getattr(resp, \"request\", None)\n",
    "        method = method or (getattr(req, \"method\", None)) or \"?\"\n",
    "        url = url or (getattr(req, \"url\", None)) or \"?\"\n",
    "    except Exception:\n",
    "        method = method or \"?\"\n",
    "        url = url or \"?\"\n",
    "\n",
    "    body_text = None\n",
    "    try:\n",
    "        body_text = resp.text\n",
    "    except Exception:\n",
    "        body_text = \"<no text>\"\n",
    "\n",
    "    detail = _json_preview(body_text)\n",
    "\n",
    "    msg_lines = [\n",
    "        f\"HTTP {resp.status_code} {resp.reason or ''} while {method} {url}\",\n",
    "    ]\n",
    "    if context:\n",
    "        msg_lines.append(f\"Context: {context}\")\n",
    "    if params:\n",
    "        msg_lines.append(f\"Params: {params}\")\n",
    "    if json_body is not None:\n",
    "        msg_lines.append(f\"JSON: {json_body}\")\n",
    "    if data is not None and json_body is None:\n",
    "        msg_lines.append(f\"Data: {str(data)[:1000]}\")\n",
    "    if auth is not None:\n",
    "        msg_lines.append(f\"Auth: {_redact_auth(auth)}\")\n",
    "    try:\n",
    "        msg_lines.append(f\"Response headers: {dict(resp.headers)})\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    msg_lines.append(\"Response body preview:\" + detail)\n",
    "\n",
    "    import requests as _requests\n",
    "    err = _requests.HTTPError(\"\\n\".join(msg_lines), response=resp)\n",
    "    raise err\n",
    "\n",
    "# --- API ensure helpers ---\n",
    "def ensure_topic(name: str) -> Optional[int]:\n",
    "    name = name.strip()\n",
    "    if not name:\n",
    "        return None\n",
    "    if name in topic_cache:\n",
    "        return topic_cache[name]\n",
    "    resp = requests.get(f\"{BASE_URL}/api/topics/\", params={'topic': name}, auth=AUTH)\n",
    "    raise_for_status_detailed(resp, method='GET', url=f\"{BASE_URL}/api/topics/\", params={'topic': name}, auth=AUTH, context='ensure_topic:get')\n",
    "    results = _extract_results(resp.json())\n",
    "    if results:\n",
    "        topic_cache[name] = results[0]['id']\n",
    "        return topic_cache[name]\n",
    "    resp = requests.post(f\"{BASE_URL}/api/topics/\", json={'topic': name}, auth=AUTH)\n",
    "    raise_for_status_detailed(resp, method='POST', url=f\"{BASE_URL}/api/topics/\", json_body={'topic': name}, auth=AUTH, context='ensure_topic:create')\n",
    "    topic_cache[name] = resp.json()['id']\n",
    "    return topic_cache[name]\n",
    "\n",
    "def ensure_baselemma(base_it: str, base_en: str) -> int:\n",
    "    key = (base_it, base_en)\n",
    "    if key in baselemma_cache:\n",
    "        return baselemma_cache[key]\n",
    "    resp = requests.get(\n",
    "        f\"{BASE_URL}/api/baselemmas/\",\n",
    "        params={'base_lemma_italian': base_it, 'base_lemma_english': base_en},\n",
    "        auth=AUTH,\n",
    "    )\n",
    "    raise_for_status_detailed(resp, method='GET', url=f\"{BASE_URL}/api/baselemmas/\", params={'base_lemma_italian': base_it, 'base_lemma_english': base_en}, auth=AUTH, context='ensure_baselemma:get')\n",
    "    results = _extract_results(resp.json())\n",
    "    if results:\n",
    "        baselemma_cache[key] = results[0]['id']\n",
    "        return baselemma_cache[key]\n",
    "    resp = requests.post(\n",
    "        f\"{BASE_URL}/api/baselemmas/\",\n",
    "        json={'base_lemma_italian': base_it, 'base_lemma_english': base_en},\n",
    "        auth=AUTH,\n",
    "    )\n",
    "    raise_for_status_detailed(resp, method='POST', url=f\"{BASE_URL}/api/baselemmas/\", json_body={'base_lemma_italian': base_it, 'base_lemma_english': base_en}, auth=AUTH, context='ensure_baselemma:create')\n",
    "    baselemma_cache[key] = resp.json()['id']\n",
    "    return baselemma_cache[key]\n",
    "\n",
    "def ensure_term(term_it: str, audio_hash: str, pronunciation: str) -> int:\n",
    "    if term_it in term_cache:\n",
    "        return term_cache[term_it]\n",
    "    resp = requests.get(f\"{BASE_URL}/api/terms/\", params={'term_italian': term_it}, auth=AUTH)\n",
    "    raise_for_status_detailed(resp, method='GET', url=f\"{BASE_URL}/api/terms/\", params={'term_italian': term_it}, auth=AUTH, context='ensure_term:get')\n",
    "    results = _extract_results(resp.json())\n",
    "    if results:\n",
    "        term_cache[term_it] = results[0]['id']\n",
    "        return term_cache[term_it]\n",
    "    body = {'term_italian': term_it}\n",
    "    if audio_hash:\n",
    "        body['audio_hash_italian'] = audio_hash\n",
    "    if pronunciation:\n",
    "        body['pronunciation'] = pronunciation\n",
    "    resp = requests.post(f\"{BASE_URL}/api/terms/\", json=body, auth=AUTH)\n",
    "    raise_for_status_detailed(resp, method='POST', url=f\"{BASE_URL}/api/terms/\", json_body=body, auth=AUTH, context='ensure_term:create')\n",
    "    term_cache[term_it] = resp.json()['id']\n",
    "    return term_cache[term_it]\n",
    "\n",
    "def ensure_baselemma_topic(base_lemma_id: int, topic_id: Optional[int]) -> None:\n",
    "    if topic_id is None:\n",
    "        return\n",
    "    key = (base_lemma_id, topic_id)\n",
    "    if key in baselemma_topic_cache:\n",
    "        return\n",
    "    resp = requests.post(\n",
    "        f\"{BASE_URL}/api/baselemma-topics/\",\n",
    "        json={'base_lemma': base_lemma_id, 'topic': topic_id},\n",
    "        auth=AUTH,\n",
    "    )\n",
    "    if resp.status_code not in (200, 201):\n",
    "        raise_for_status_detailed(resp, method='POST', url=f\"{BASE_URL}/api/baselemma-topics/\", json_body={'base_lemma': base_lemma_id, 'topic': topic_id}, auth=AUTH, context='ensure_baselemma_topic:create')\n",
    "    baselemma_topic_cache.add(key)\n",
    "\n",
    "def ensure_term_translation(payload: Dict[str, Any]) -> int:\n",
    "    params = {\n",
    "        'term': payload['term'],\n",
    "        'base_lemma': payload['base_lemma'],\n",
    "        'translation_english': payload['translation_english'],\n",
    "        'part_of_speech': payload['part_of_speech'],\n",
    "    }\n",
    "    resp = requests.get(f\"{BASE_URL}/api/term-translations/\", params=params, auth=AUTH)\n",
    "    raise_for_status_detailed(resp, method='GET', url=f\"{BASE_URL}/api/term-translations/\", params=params, auth=AUTH, context='ensure_term_translation:get')\n",
    "    existing = _extract_results(resp.json())\n",
    "    if existing:\n",
    "        return existing[0]['id']\n",
    "    resp = requests.post(f\"{BASE_URL}/api/term-translations/\", json=payload, auth=AUTH)\n",
    "    raise_for_status_detailed(resp, method='POST', url=f\"{BASE_URL}/api/term-translations/\", json_body=payload, auth=AUTH, context='ensure_term_translation:create')\n",
    "    return resp.json()['id']\n",
    "\n",
    "# --- Utility to collect CSV files ---\n",
    "def iter_csv_files(root_dir: str):\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for fn in filenames:\n",
    "            if fn.lower().endswith('.csv'):\n",
    "                yield os.path.join(dirpath, fn)\n",
    "\n",
    "# --- Main export loop ---\n",
    "csv_files = sorted(iter_csv_files(DATA_DIR))\n",
    "print(f'Found {len(csv_files)} CSV files under {DATA_DIR}')\n",
    "created_tt_ids = []\n",
    "for csv_path in tqdm(csv_files, desc='CSV files'):\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in tqdm(reader, desc=os.path.basename(csv_path), leave=False):\n",
    "            # Basic fields\n",
    "            base_it = (row.get('base_lemma_italian') or '').strip()\n",
    "            base_en = (row.get('base_lemma_english') or '').strip()\n",
    "            term_it = (row.get('term_italian') or '').strip()\n",
    "            translation_en = (row.get('translation_english') or '').strip()\n",
    "            if not (base_it and base_en and term_it and translation_en):\n",
    "                # Skip incomplete rows\n",
    "                continue\n",
    "\n",
    "            # Ensure base lemma and term exist\n",
    "            base_lemma_id = ensure_baselemma(base_it, base_en)\n",
    "            term_id = ensure_term(\n",
    "                term_it,\n",
    "                (row.get('italian_audio_hash') or '').strip(),\n",
    "                (row.get('pronunciation') or '').strip(),\n",
    "            )\n",
    "\n",
    "            # Topics and link to base lemma\n",
    "            topics = [t.strip() for t in (row.get('topics') or '').split(',') if t.strip()]\n",
    "            for t in topics:\n",
    "                topic_id = ensure_topic(t)\n",
    "                ensure_baselemma_topic(base_lemma_id, topic_id)\n",
    "\n",
    "            # Part of speech and position within lemma group\n",
    "            pos_value = normalize_pos(row.get('part_of_speech'))\n",
    "            if not pos_value:\n",
    "                raise ValueError(f\"Unhandled part_of_speech: {row.get('part_of_speech')} in {csv_path}\")\n",
    "            key = (base_it, base_en)\n",
    "            position = position_tracker[key]\n",
    "            position_tracker[key] += 1\n",
    "\n",
    "            payload = {\n",
    "                'term': term_id,\n",
    "                'base_lemma': base_lemma_id,\n",
    "                'translation_english': translation_en,\n",
    "                'part_of_speech': pos_value,\n",
    "                'position': position,\n",
    "            }\n",
    "\n",
    "            # Optional attributes\n",
    "            gender = normalize_gender(row.get('gender'))\n",
    "            if gender:\n",
    "                payload['gender'] = gender\n",
    "            plurality = normalize_plurality(row.get('plurality'))\n",
    "            if plurality:\n",
    "                payload['plurality'] = plurality\n",
    "            if (row.get('article_type') or '').strip():\n",
    "                payload['article_type'] = row['article_type'].strip()\n",
    "            if (row.get('article_italian') or '').strip():\n",
    "                payload['article_italian'] = row['article_italian'].strip()\n",
    "            if (row.get('preposition') or '').strip():\n",
    "                payload['preposition'] = row['preposition'].strip()\n",
    "            if (row.get('example_sentence_english') or '').strip():\n",
    "                payload['example_sentence_english'] = row['example_sentence_english'].strip()\n",
    "            if (row.get('example_sentence_italian') or '').strip():\n",
    "                payload['example_sentence_italian'] = row['example_sentence_italian'].strip()\n",
    "            if (row.get('english_audio_hash') or '').strip():\n",
    "                payload['audio_hash_english'] = row['english_audio_hash'].strip()\n",
    "            if (row.get('notes') or '').strip():\n",
    "                payload['notes'] = row['notes'].strip()\n",
    "\n",
    "            tt_id = ensure_term_translation(payload)\n",
    "            created_tt_ids.append(tt_id)\n",
    "\n",
    "print(f'Created/verified {len(created_tt_ids)} term translations across {len(position_tracker)} base lemma groups.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
