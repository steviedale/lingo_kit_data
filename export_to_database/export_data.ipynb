{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nImport LingoKit base data (Topics, BaseLemmas, Terms, TermTranslations) from a CSV\\ninto your Django REST API.\\n\\nUsage:\\n  python import_lingokit_data.py     --base-url https://yourdomain.com     --csv ./data.csv     --auth token --token YOUR_DRF_OR_JWT_TOKEN\\n\\nAlternative auth:\\n  # Basic auth\\n  --auth basic --username alice --password secret\\n\\n  # Session cookie present in env/CI (already logged in)\\n  --auth session --cookie \"sessionid=...; csrftoken=...\"\\n\\nCSV columns (example provided in the prompt) are mapped 1:1 where possible.\\n\\nIdempotency:\\n- The script \"get-or-create\"s each entity using lookups:\\n  Topic: by exact `topic`\\n  BaseLemma: by exact `(base_lemma_italian, base_lemma_english)`\\n  Term: by exact `term_italian`\\n  TermTranslation: by key `(term_id, base_lemma_id, translation_english, part_of_speech)`\\n    (adjust this if your true uniqueness differs)\\n\\nEndpoints expected (adjust with CLI flags if your paths differ):\\n  /api/topics/\\n  /api/baselemmas/\\n  /api/terms/\\n  /api/term-translations/\\n  /api/baselemma-topics/   # optional; used if --link-lemma-topics=api\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Import LingoKit base data (Topics, BaseLemmas, Terms, TermTranslations) from a CSV\n",
    "into your Django REST API.\n",
    "\n",
    "Usage:\n",
    "  python import_lingokit_data.py \\\n",
    "    --base-url https://yourdomain.com \\\n",
    "    --csv ./data.csv \\\n",
    "    --auth token --token YOUR_DRF_OR_JWT_TOKEN\n",
    "\n",
    "Alternative auth:\n",
    "  # Basic auth\n",
    "  --auth basic --username alice --password secret\n",
    "\n",
    "  # Session cookie present in env/CI (already logged in)\n",
    "  --auth session --cookie \"sessionid=...; csrftoken=...\"\n",
    "\n",
    "CSV columns (example provided in the prompt) are mapped 1:1 where possible.\n",
    "\n",
    "Idempotency:\n",
    "- The script \"get-or-create\"s each entity using lookups:\n",
    "  Topic: by exact `topic`\n",
    "  BaseLemma: by exact `(base_lemma_italian, base_lemma_english)`\n",
    "  Term: by exact `term_italian`\n",
    "  TermTranslation: by key `(term_id, base_lemma_id, translation_english, part_of_speech)`\n",
    "    (adjust this if your true uniqueness differs)\n",
    "\n",
    "Endpoints expected (adjust with CLI flags if your paths differ):\n",
    "  /api/topics/\n",
    "  /api/baselemmas/\n",
    "  /api/terms/\n",
    "  /api/term-translations/\n",
    "  /api/baselemma-topics/   # optional; used if --link-lemma-topics=api\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "\n",
    "def booly(v: str) -> Optional[bool]:\n",
    "    if v is None:\n",
    "        return None\n",
    "    s = str(v).strip().lower()\n",
    "    if s in (\"true\", \"1\", \"yes\", \"y\"):\n",
    "        return True\n",
    "    if s in (\"false\", \"0\", \"no\", \"n\"):\n",
    "        return False\n",
    "    return None\n",
    "\n",
    "def empty_to_none(v: str) -> Optional[str]:\n",
    "    if v is None:\n",
    "        return None\n",
    "    v = str(v).strip()\n",
    "    return v if v else None\n",
    "\n",
    "def split_csv_list(v: str) -> List[str]:\n",
    "    if not v:\n",
    "        return []\n",
    "    # topics field expected like: \"quantity,manner\"\n",
    "    return [t.strip() for t in v.split(\",\") if t.strip()]\n",
    "\n",
    "def die(msg: str, code: int = 2):\n",
    "    print(f\"ERROR: {msg}\", file=sys.stderr)\n",
    "    sys.exit(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# API Client\n",
    "# -----------------------\n",
    "class Api:\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_url: str,\n",
    "        auth_mode: str,\n",
    "        token: Optional[str] = None,\n",
    "        username: Optional[str] = None,\n",
    "        password: Optional[str] = None,\n",
    "        cookie: Optional[str] = None,\n",
    "        paths: Dict[str, str] = None,\n",
    "        timeout: int = 30,\n",
    "    ):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.auth_mode = auth_mode\n",
    "        self.session = requests.Session()\n",
    "        self.timeout = timeout\n",
    "\n",
    "        # default paths (adjust via CLI if needed)\n",
    "        self.paths = {\n",
    "            \"topics\": \"/api/topics/\",\n",
    "            \"baselemmas\": \"/api/baselemmas/\",\n",
    "            \"terms\": \"/api/terms/\",\n",
    "            \"term_translations\": \"/api/term-translations/\",\n",
    "            \"baselemma_topics\": \"/api/baselemma-topics/\",  # optional\n",
    "        }\n",
    "        if paths:\n",
    "            self.paths.update(paths)\n",
    "\n",
    "        # Auth headers / cookies\n",
    "        if auth_mode == \"token\":\n",
    "            # Works for DRF tokens (Token <key>) or JWT (Bearer <jwt>)\n",
    "            # Try to detect format â€“ you can force with --token-prefix\n",
    "            if token is None:\n",
    "                die(\"token auth selected but no --token provided\")\n",
    "            # Heuristic: if token has dots, treat as JWT\n",
    "            if \".\" in token:\n",
    "                self.session.headers[\"Authorization\"] = f\"Bearer {token}\"\n",
    "            else:\n",
    "                self.session.headers[\"Authorization\"] = f\"Token {token}\"\n",
    "        elif auth_mode == \"basic\":\n",
    "            if not username or not password:\n",
    "                die(\"basic auth selected but missing --username/--password\")\n",
    "            self.session.auth = (username, password)\n",
    "        elif auth_mode == \"session\":\n",
    "            if not cookie:\n",
    "                die(\"session auth selected but missing --cookie\")\n",
    "            self.session.headers[\"Cookie\"] = cookie\n",
    "        else:\n",
    "            die(f\"Unknown auth mode: {auth_mode}\")\n",
    "\n",
    "        self.session.headers[\"Accept\"] = \"application/json\"\n",
    "        # Allow either form-encoded or JSON. We'll use JSON by default.\n",
    "        self.session.headers[\"Content-Type\"] = \"application/json\"\n",
    "\n",
    "    def url(self, key: str) -> str:\n",
    "        return self.base_url + self.paths[key]\n",
    "\n",
    "    def get(self, key: str, params: Dict[str, Any]) -> requests.Response:\n",
    "        return self.session.get(self.url(key), params=params, timeout=self.timeout)\n",
    "\n",
    "    def post(self, key: str, payload: Dict[str, Any]) -> requests.Response:\n",
    "        return self.session.post(self.url(key), data=json.dumps(payload), timeout=self.timeout)\n",
    "\n",
    "    # Generic paginate-through helper (DRF-style pagination optional)\n",
    "    def list_all(self, key: str, params: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        items = []\n",
    "        url = self.url(key)\n",
    "        while True:\n",
    "            resp = self.session.get(url, params=params, timeout=self.timeout)\n",
    "            if resp.status_code != 200:\n",
    "                raise RuntimeError(f\"GET {url} failed: {resp.status_code} {resp.text}\")\n",
    "            data = resp.json()\n",
    "            if isinstance(data, dict) and \"results\" in data:\n",
    "                items.extend(data[\"results\"])\n",
    "                url = data.get(\"next\")\n",
    "                if not url:\n",
    "                    break\n",
    "                params = {}  # next already encodes query\n",
    "            elif isinstance(data, list):\n",
    "                items.extend(data)\n",
    "                break\n",
    "            else:\n",
    "                # Non-standard shape; just return as-is\n",
    "                if isinstance(data, dict):\n",
    "                    items.append(data)\n",
    "                break\n",
    "        return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Upsert functions\n",
    "# -----------------------\n",
    "\n",
    "def upsert_topic(api: Api, topic: str, cache: Dict[str, int]) -> int:\n",
    "    if topic in cache:\n",
    "        return cache[topic]\n",
    "\n",
    "    # Try GET by exact name (adjust the filter param if your API differs)\n",
    "    resp = api.get(\"topics\", params={\"topic\": topic})\n",
    "    if resp.status_code == 200:\n",
    "        data = resp.json()\n",
    "        # handle both list and DRF-paginated\n",
    "        if isinstance(data, list):\n",
    "            for t in data:\n",
    "                if t.get(\"topic\") == topic:\n",
    "                    cache[topic] = t[\"id\"]\n",
    "                    return t[\"id\"]\n",
    "        elif isinstance(data, dict):\n",
    "            results = data.get(\"results\", [])\n",
    "            for t in results:\n",
    "                if t.get(\"topic\") == topic:\n",
    "                    cache[topic] = t[\"id\"]\n",
    "                    return t[\"id\"]\n",
    "\n",
    "    # Create\n",
    "    resp = api.post(\"topics\", {\"topic\": topic})\n",
    "    if resp.status_code not in (200, 201):\n",
    "        raise RuntimeError(f\"Create topic '{topic}' failed: {resp.status_code} {resp.text}\")\n",
    "    tid = resp.json()[\"id\"]\n",
    "    cache[topic] = tid\n",
    "    return tid\n",
    "\n",
    "def upsert_baselemma(api: Api, bl_it: str, bl_en: str, cache: Dict[Tuple[str, str], int]) -> int:\n",
    "    key = (bl_it, bl_en)\n",
    "    if key in cache:\n",
    "        return cache[key]\n",
    "\n",
    "    resp = api.get(\"baselemmas\", params={\n",
    "        \"base_lemma_italian\": bl_it,\n",
    "        \"base_lemma_english\": bl_en\n",
    "    })\n",
    "    if resp.status_code == 200:\n",
    "        data = resp.json()\n",
    "        items = data.get(\"results\", data if isinstance(data, list) else [])\n",
    "        for b in items:\n",
    "            if b.get(\"base_lemma_italian\") == bl_it and b.get(\"base_lemma_english\") == bl_en:\n",
    "                cache[key] = b[\"id\"]\n",
    "                return b[\"id\"]\n",
    "\n",
    "    resp = api.post(\"baselemmas\", {\n",
    "        \"base_lemma_italian\": bl_it,\n",
    "        \"base_lemma_english\": bl_en\n",
    "    })\n",
    "    if resp.status_code not in (200, 201):\n",
    "        raise RuntimeError(f\"Create BaseLemma '{bl_it} / {bl_en}' failed: {resp.status_code} {resp.text}\")\n",
    "    bid = resp.json()[\"id\"]\n",
    "    cache[key] = bid\n",
    "    return bid\n",
    "\n",
    "def upsert_term(api: Api, term_it: str, cache: Dict[str, int], audio_hash_italian: Optional[str], pronunciation: Optional[str]) -> int:\n",
    "    if term_it in cache:\n",
    "        return cache[term_it]\n",
    "\n",
    "    resp = api.get(\"terms\", params={\"term_italian\": term_it})\n",
    "    if resp.status_code == 200:\n",
    "        data = resp.json()\n",
    "        items = data.get(\"results\", data if isinstance(data, list) else [])\n",
    "        for t in items:\n",
    "            if t.get(\"term_italian\") == term_it:\n",
    "                cache[term_it] = t[\"id\"]\n",
    "                return t[\"id\"]\n",
    "\n",
    "    payload = {\n",
    "        \"term_italian\": term_it,\n",
    "        \"audio_hash_italian\": audio_hash_italian,\n",
    "        \"pronunciation\": pronunciation,\n",
    "    }\n",
    "    resp = api.post(\"terms\", payload)\n",
    "    if resp.status_code not in (200, 201):\n",
    "        raise RuntimeError(f\"Create Term '{term_it}' failed: {resp.status_code} {resp.text}\")\n",
    "    tid = resp.json()[\"id\"]\n",
    "    cache[term_it] = tid\n",
    "    return tid\n",
    "\n",
    "def upsert_baselemma_topic_link(api: Api, baselemma_id: int, topic_id: int, mode: str):\n",
    "    \"\"\"\n",
    "    mode = 'api' -> POST to /api/baselemma-topics/\n",
    "    mode = 'skip' -> do nothing (e.g., your API auto-links on BaseLemma save)\n",
    "    \"\"\"\n",
    "    if mode == \"skip\":\n",
    "        return\n",
    "    payload = {\"base_lemma\": baselemma_id, \"topic\": topic_id}\n",
    "    resp = api.post(\"baselemma_topics\", payload)\n",
    "    # Allow both 201 created and 200/409 for already exists\n",
    "    if resp.status_code not in (200, 201, 409):\n",
    "        raise RuntimeError(f\"Link BaseLemma({baselemma_id}) to Topic({topic_id}) failed: {resp.status_code} {resp.text}\")\n",
    "\n",
    "def upsert_term_translation(\n",
    "    api: Api,\n",
    "    term_id: int,\n",
    "    baselemma_id: int,\n",
    "    tt_payload: Dict[str, Any],\n",
    "    unique_key=(\"translation_english\", \"part_of_speech\"),\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Upsert TermTranslation by (term, base_lemma, translation_english, part_of_speech).\n",
    "    Adjust `unique_key` to match your actual uniqueness constraints if needed.\n",
    "    \"\"\"\n",
    "    # Try to find existing\n",
    "    params = {\n",
    "        \"term\": term_id,\n",
    "        \"base_lemma\": baselemma_id,\n",
    "    }\n",
    "    for k in unique_key:\n",
    "        params[k] = tt_payload.get(k)\n",
    "\n",
    "    resp = api.get(\"term_translations\", params=params)\n",
    "    if resp.status_code == 200:\n",
    "        data = resp.json()\n",
    "        items = data.get(\"results\", data if isinstance(data, list) else [])\n",
    "        if items:\n",
    "            return items[0][\"id\"]\n",
    "\n",
    "    payload = {\"term\": term_id, \"base_lemma\": baselemma_id}\n",
    "    payload.update(tt_payload)\n",
    "    resp = api.post(\"term_translations\", payload)\n",
    "    if resp.status_code not in (200, 201):\n",
    "        raise RuntimeError(\n",
    "            f\"Create TermTranslation failed ({term_id}, {baselemma_id}): {resp.status_code} {resp.text}\"\n",
    "        )\n",
    "    return resp.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Row mapping\n",
    "# -----------------------\n",
    "\n",
    "def map_row_to_models(row: Dict[str, str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Map raw CSV row to model payloads & linkage.\n",
    "    Returns dict with:\n",
    "      - topics: List[str]\n",
    "      - baselemma: {base_lemma_italian, base_lemma_english}\n",
    "      - term: {term_italian, audio_hash_italian?, pronunciation?}\n",
    "      - term_translation: dict of TT fields\n",
    "    \"\"\"\n",
    "    topics = split_csv_list(row.get(\"topics\"))\n",
    "\n",
    "    baselemma = {\n",
    "        \"base_lemma_italian\": row.get(\"base_lemma_italian\", \"\").strip(),\n",
    "        \"base_lemma_english\": row.get(\"base_lemma_english\", \"\").strip(),\n",
    "    }\n",
    "\n",
    "    term = {\n",
    "        \"term_italian\": row.get(\"term_italian\", \"\").strip(),\n",
    "        \"audio_hash_italian\": empty_to_none(row.get(\"italian_audio_hash\")),\n",
    "        \"pronunciation\": empty_to_none(row.get(\"pronunciation\")),  # optional column\n",
    "    }\n",
    "\n",
    "    # Map TT fields (respect your model names)\n",
    "    tt = {\n",
    "        \"translation_english\": empty_to_none(row.get(\"translation_english\")),\n",
    "        \"part_of_speech\": empty_to_none(row.get(\"part_of_speech\")),\n",
    "        \"gender\": empty_to_none(row.get(\"gender\")),\n",
    "        \"plurality\": empty_to_none(row.get(\"plurality\")),\n",
    "        \"article_type\": empty_to_none(row.get(\"article_type\")),\n",
    "        \"article_italian\": empty_to_none(row.get(\"article_italian\")),\n",
    "        \"preposition\": empty_to_none(row.get(\"preposition\")),\n",
    "        \"notes\": empty_to_none(row.get(\"notes\")),\n",
    "        \"example_sentence_italian\": empty_to_none(row.get(\"example_sentence_italian\")),\n",
    "        \"example_sentence_english\": empty_to_none(row.get(\"example_sentence_english\")),\n",
    "        \"audio_hash_english\": empty_to_none(row.get(\"english_audio_hash\")),\n",
    "        # Optional fields in CSV that don't map to the provided TermTranslation model\n",
    "        # are read but ignored unless you expand your model:\n",
    "        # subtype, person_number, mood, tense, is_comparative, is_superlative, is_compound,\n",
    "        # added_particle_italian\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"topics\": topics,\n",
    "        \"baselemma\": baselemma,\n",
    "        \"term\": term,\n",
    "        \"term_translation\": tt,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Main import routine\n",
    "# -----------------------\n",
    "\n",
    "def import_csv(\n",
    "    api: Api,\n",
    "    csv_path: Path,\n",
    "    link_lemma_topics_mode: str = \"api\",  # 'api' or 'skip'\n",
    "):\n",
    "    # Caches to avoid redundant GETs/POSTs\n",
    "    topic_cache: Dict[str, int] = {}\n",
    "    baselemma_cache: Dict[Tuple[str, str], int] = {}\n",
    "    term_cache: Dict[str, int] = {}\n",
    "\n",
    "    created_counts = {\n",
    "        \"topics\": 0,\n",
    "        \"baselemmas\": 0,\n",
    "        \"terms\": 0,\n",
    "        \"term_translations\": 0,\n",
    "        \"links\": 0,\n",
    "        \"rows\": 0,\n",
    "    }\n",
    "\n",
    "    with csv_path.open(newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for idx, row in enumerate(reader, start=1):\n",
    "            mapped = map_row_to_models(row)\n",
    "\n",
    "            # Validate minimal required values\n",
    "            bl_it = mapped[\"baselemma\"][\"base_lemma_italian\"]\n",
    "            bl_en = mapped[\"baselemma\"][\"base_lemma_english\"]\n",
    "            term_it = mapped[\"term\"][\"term_italian\"]\n",
    "            if not bl_it or not bl_en or not term_it:\n",
    "                print(f\"[row {idx}] Skipping: missing base lemma or term_italian\")\n",
    "                continue\n",
    "\n",
    "            # Upserts\n",
    "            # 1) Topics\n",
    "            topic_ids = []\n",
    "            for t in mapped[\"topics\"]:\n",
    "                prior = topic_cache.get(t)\n",
    "                tid = upsert_topic(api, t, topic_cache)\n",
    "                if prior is None and tid:\n",
    "                    created_counts[\"topics\"] += 1\n",
    "                topic_ids.append(tid)\n",
    "\n",
    "            # 2) BaseLemma\n",
    "            prior = baselemma_cache.get((bl_it, bl_en))\n",
    "            baselemma_id = upsert_baselemma(api, bl_it, bl_en, baselemma_cache)\n",
    "            if prior is None:\n",
    "                created_counts[\"baselemmas\"] += 1\n",
    "\n",
    "            # 3) Link BaseLemma <-> Topics\n",
    "            for tid in topic_ids:\n",
    "                upsert_baselemma_topic_link(api, baselemma_id, tid, link_lemma_topics_mode)\n",
    "                created_counts[\"links\"] += 1\n",
    "\n",
    "            # 4) Term\n",
    "            prior = term_cache.get(term_it)\n",
    "            term_id = upsert_term(\n",
    "                api,\n",
    "                term_it,\n",
    "                term_cache,\n",
    "                mapped[\"term\"][\"audio_hash_italian\"],\n",
    "                mapped[\"term\"][\"pronunciation\"],\n",
    "            )\n",
    "            if prior is None:\n",
    "                created_counts[\"terms\"] += 1\n",
    "\n",
    "            # 5) TermTranslation\n",
    "            tt_payload = mapped[\"term_translation\"]\n",
    "            if not tt_payload.get(\"translation_english\") or not tt_payload.get(\"part_of_speech\"):\n",
    "                print(f\"[row {idx}] Skipping TermTranslation: requires translation_english and part_of_speech\")\n",
    "                continue\n",
    "\n",
    "            tt_id = upsert_term_translation(\n",
    "                api,\n",
    "                term_id=term_id,\n",
    "                baselemma_id=baselemma_id,\n",
    "                tt_payload=tt_payload,\n",
    "            )\n",
    "            if tt_id:\n",
    "                created_counts[\"term_translations\"] += 1\n",
    "            created_counts[\"rows\"] += 1\n",
    "\n",
    "            if idx % 200 == 0:\n",
    "                print(f\"Processed {idx} rows...\")\n",
    "\n",
    "    print(\"Done.\")\n",
    "    print(json.dumps(created_counts, indent=2))\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"Import LingoKit data into Django via REST API\")\n",
    "\n",
    "    p.add_argument(\"--base-url\", required=True, help=\"Base URL like https://example.com\")\n",
    "    p.add_argument(\"--csv\", required=True, help=\"Path to CSV file\")\n",
    "\n",
    "    # Auth\n",
    "    p.add_argument(\"--auth\", choices=[\"token\", \"basic\", \"session\"], default=\"token\")\n",
    "    p.add_argument(\"--token\", help=\"DRF token or JWT (auto-detected)\")\n",
    "    p.add_argument(\"--username\")\n",
    "    p.add_argument(\"--password\")\n",
    "    p.add_argument(\"--cookie\", help='Entire Cookie header, e.g. \"sessionid=...; csrftoken=...\"')\n",
    "\n",
    "    # Endpoint path overrides (if your API uses different paths)\n",
    "    p.add_argument(\"--topics-path\", default=\"/api/topics/\")\n",
    "    p.add_argument(\"--baselemmas-path\", default=\"/api/baselemmas/\")\n",
    "    p.add_argument(\"--terms-path\", default=\"/api/terms/\")\n",
    "    p.add_argument(\"--term-translations-path\", default=\"/api/term-translations/\")\n",
    "    p.add_argument(\"--baselemma-topics-path\", default=\"/api/baselemma-topics/\")\n",
    "\n",
    "    # How to link BaseLemma<->Topic\n",
    "    p.add_argument(\"--link-lemma-topics\", choices=[\"api\", \"skip\"], default=\"api\",\n",
    "                   help=\"Use 'api' if you expose /api/baselemma-topics/; 'skip' if your backend auto-links via BaseLemma payload.\")\n",
    "\n",
    "    args = p.parse_args()\n",
    "\n",
    "    paths = {\n",
    "        \"topics\": args.topics_path,\n",
    "        \"baselemmas\": args.baselemmas_path,\n",
    "        \"terms\": args.terms_path,\n",
    "        \"term_translations\": args.term_translations_path,\n",
    "        \"baselemma_topics\": args.baselemma_topics_path,\n",
    "    }\n",
    "\n",
    "    return args, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args, paths = parse_args()\n",
    "csv_path = Path(args.csv)\n",
    "if not csv_path.exists():\n",
    "    die(f\"CSV does not exist: {csv_path}\")\n",
    "\n",
    "api = Api(\n",
    "    base_url=args.base_url,\n",
    "    auth_mode=args.auth,\n",
    "    token=args.token,\n",
    "    username=args.username,\n",
    "    password=args.password,\n",
    "    cookie=args.cookie,\n",
    "    paths=paths,\n",
    ")\n",
    "import_csv(api, csv_path, link_lemma_topics_mode=args.link_lemma_topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
